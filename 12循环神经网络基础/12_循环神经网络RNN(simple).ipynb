{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9134061f",
   "metadata": {},
   "source": [
    "### RNN Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b9194",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "模块名称  : 作用\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$x_t$:\n",
    "    表时刻$t$时的输入数据\n",
    "\n",
    "\n",
    "RNN Cell:\n",
    "本质上是一个线性层\n",
    "\n",
    "\n",
    "$h_t$:\n",
    "表时刻$t$时得到的输出（隐含层）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388d3d7",
   "metadata": {},
   "source": [
    "RNN Cell为一个线性层Linear，在$t$时刻下的$N$维向量，经过Cell后即可变为一个$M$维的向量$h_t$,而与其他线性层不同，RNN Cell为一个共享的线性层。即重复利用，权重共享。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34d293",
   "metadata": {},
   "source": [
    "由于$x_1 \\cdots x_n$，为一组序列信息，每一个$x_i$都至少应包含$x_{i-1}$的信息。也就是说，针对$x_2$的操作所得到的$h_2$中，应当包含$x_1$的信息，因此在设计中，把$x_1$处理后得到的$h_1$一并向下传递。\n",
    "$h_0$是一种前置信息。例如若实现图像到文本的转换，可以利用CNN+FC(全连接层)对图像进行操作，再将输出的向量作为$h_0$参与到RNN的运算中。\n",
    "若没有可获知的前置信息，可将$h_0$设置为与$x_i$同维度的零向量。\n",
    "RNN Cell为同一个Linear，即让设计好的Linear反复参与运算，实现权重共享。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e276a",
   "metadata": {},
   "source": [
    "---\n",
    "## RNN Cell 计算过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5841f73a",
   "metadata": {},
   "source": [
    "符号 :\n",
    "    标注\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$h_t$ $h_{t-1}$\n",
    ":    隐藏层hidden的结果向量\n",
    "\n",
    "\n",
    "$x_t$\n",
    ":    输入层的输入向量\n",
    "\n",
    "\n",
    "$R ^{hidden\\_size}$\n",
    ":表隐藏层的向量维度\n",
    "\n",
    "\n",
    "$R^{input\\_size}$\n",
    ":表输入层的向量维度\n",
    "\n",
    "\n",
    "$W_{ih}$\n",
    ":用于计算输入的权重，维度大小为$hidden\\_size \\times input\\_size$\n",
    "\n",
    "\n",
    "$b_{ih}$\n",
    ":用于计算输入时的偏置量\n",
    "\n",
    "\n",
    "$W_{hh}$\n",
    ":用于计算隐藏层的权重，维度大小为$hidden\\_size \\times hidden\\_size$\n",
    "\n",
    "\n",
    "$b_{hh}$\n",
    ":用于计算隐藏层时的偏置量\n",
    "\n",
    "\n",
    "tanh\n",
    ":激活函数，值域为$ (-1, +1)$\n",
    "\n",
    "\n",
    "+\n",
    ":求和模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e9bfe",
   "metadata": {},
   "source": [
    "---\n",
    "在RNN计算过程中，分别对输入$x_t$以及前文的隐藏层输出$h_{t-1}$进行线性计算，再进行求和，对所得到的一维向量，利用tanh激活函数进行激活，由此可以得到当前隐藏层的输出$h_t$,其计算过程如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06016c0",
   "metadata": {},
   "source": [
    "$h_t = tanh(W_{ih}x_t + b_{ih} + W_{hh}h_{t-1} + b_{hh})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59697c9",
   "metadata": {},
   "source": [
    "实际上RNN Cell的计算过程中为线性计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4346d0b6",
   "metadata": {},
   "source": [
    "$W_{hh}h_{t-1}+W_{ih}x_{t} = \n",
    "\\begin{bmatrix}\n",
    "{W_{hh}}&{W_{ih}}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "h\\\\\n",
    "x\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec197b2e",
   "metadata": {},
   "source": [
    "即在实际运算的过程中，这两部分是拼接到一起形成矩阵再计算求和的，最终形成一个大小为$hidden\\_size \\times 1$的张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40db415a",
   "metadata": {},
   "source": [
    "---\n",
    "## 代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a843a0",
   "metadata": {},
   "source": [
    "代码实现有两种模式，一是实现自己的RNN Cell，再自己重写循环调用等逻辑。二是直接调用RNN的网络。\n",
    "\n",
    "### 重点在于控制其输入输出的维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b49d4fd",
   "metadata": {},
   "source": [
    "## RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06df32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1          # batchsize\n",
    "seq_len = 3             # 序列长度为3（样本个数） ex：x1,x2,x3\n",
    "input_size = 4          # 输入维度               ex: x1,x2,x3都是四维向量\n",
    "hidden_size =2          # 隐藏层为2              ex：每一个hidden都是一个2维向量\n",
    "\n",
    "#输入维度input_size,隐藏层维度hidden_size\n",
    "cell = torch.nn.RNNCell(input_size = input_size, hidden_size = hidden_size)    # 构造RNNCell，分别表示两个输入的维度（就可以确定权重的维度和偏置的维度） \n",
    "#维度最重要\n",
    "dataset = torch.randn(seq_len,batch_size,input_size)   # 整个序列维度（序列的长度 批量 输入x的维度）（序列的长度第一个是将来循环时，每次要先拿出当前时刻t的一组张量（BATCH_SIZE, INPUT_SIZE)）\n",
    "#初始化时设为零向量\n",
    "hidden = torch.zeros(batch_size, hidden_size)          # 隐藏层初始全0  \n",
    "\n",
    "for idx,input in enumerate(dataset):\n",
    "    print('=' * 20,idx,'=' * 20)\n",
    "    print('Input size:', input.shape)\n",
    "    #输入的input 的维度（B*input_size）, hidden的维度（B*hidden_size）\n",
    "    #输出的hidden维度（B*hidden_szie）\n",
    "    hidden = cell(input, hidden)      # 当前时刻的输入和当前的hidden\n",
    "\n",
    "    print('outputs size: ', hidden.shape)\n",
    "    print(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b290bfe",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984eff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 1          # batchsize\n",
    "seq_len = 5             # 序列长度为3（样本个数） ex：x1,x2,x3\n",
    "input_size = 4          # 输入维度               ex: x1,x2,x3都是四维向量\n",
    "hidden_size =2          # 隐藏层为2              ex：每一个hidden都是一个2维向量\n",
    "num_layers = 3          # RNN有多少层\n",
    "\n",
    "#其他参数\n",
    "#batch_first=True 维度从(SeqLen*Batch*input_size)变为（Batch*SeqLen*input_size）\n",
    "#说明input维度，hidden维度，以及RNN层数\n",
    "#RNN计算耗时大，不建议层数过深\n",
    "cell = torch.nn.RNN(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers)       # 最后是RNN有多少层\n",
    "#inputs指的是X1……Xn的整个输入序列\n",
    "#hidden指的是前置条件H0\n",
    "#out指的是每一次迭代的H1……Hn隐藏层序列\n",
    "#hidden(跟out一起的)指的是最后一次迭代得到输出Hn\n",
    "inputs = torch.randn(seq_len, batch_size, input_size)          # （序列长度 批量 输入维度）\n",
    "hidden = torch.zeros(num_layers, batch_size, hidden_size)      # （numLayers 批量 隐藏层维度）\n",
    "\n",
    "out, hidden = cell(inputs, hidden)    # 右边：dataset表示整个输入序列，hidden就是开始h0\n",
    "                                      # 左边：out是h1-hn,hidden就是hn\n",
    "print(\"Output size: \", out.shape)\n",
    "print(\"Output: \", out)\n",
    "print(\"Hidden size: \", hidden.shape)\n",
    "print(\"Hidden: \", hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac99a4b",
   "metadata": {},
   "source": [
    "---\n",
    "# 参考实例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755183f6",
   "metadata": {},
   "source": [
    "现在有一个序列到序列$（seq \\to seq）$的任务，比如将“hello”转换为“ohlol”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef419ce",
   "metadata": {},
   "source": [
    "## RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fef9495d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string: loooo, Epoch [1/15] loss = 1.2609\n",
      "Predicted string: lolll, Epoch [2/15] loss = 0.8008\n",
      "Predicted string: lllll, Epoch [3/15] loss = 0.5528\n",
      "Predicted string: lllll, Epoch [4/15] loss = 0.4354\n",
      "Predicted string: lllll, Epoch [5/15] loss = 0.3854\n",
      "Predicted string: lllll, Epoch [6/15] loss = 0.3640\n",
      "Predicted string: lllll, Epoch [7/15] loss = 0.3540\n",
      "Predicted string: lllll, Epoch [8/15] loss = 0.3489\n",
      "Predicted string: lllll, Epoch [9/15] loss = 0.3461\n",
      "Predicted string: lllll, Epoch [10/15] loss = 0.3444\n",
      "Predicted string: lllll, Epoch [11/15] loss = 0.3433\n",
      "Predicted string: lllll, Epoch [12/15] loss = 0.3426\n",
      "Predicted string: lllll, Epoch [13/15] loss = 0.3422\n",
      "Predicted string: lllll, Epoch [14/15] loss = 0.3418\n",
      "Predicted string: lllll, Epoch [15/15] loss = 0.3416\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 4\n",
    "batch_size = 1\n",
    "num_layers = 1\n",
    "seq_len = 5\n",
    "\n",
    "#构建输入输出字典\n",
    "idx2char = ['e', 'h', 'l', 'o'] # 字典\n",
    "x_data = [1, 0, 2, 2, 3] # hello\n",
    "y_data = [3, 1, 2, 3, 2] # ohlol\n",
    "# y_data = [3, 1, 2, 2, 3]\n",
    "\n",
    "one_hot_lookup = [[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]\n",
    "\n",
    "#构造独热向量，此时向量维度为(SeqLen*InputSize)\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
    "#view(-1……)保留原始SeqLen，并添加batch_size,input_size两个维度\n",
    "inputs = torch.Tensor(x_one_hot).view(-1, batch_size, input_size)\n",
    "#将labels转换为（SeqLen*1）的维度\n",
    "labels = torch.LongTensor(y_data).view(-1, 1)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.rnncell = torch.nn.RNNCell(input_size = self.input_size,\n",
    "                                        hidden_size = self.hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # RNNCell input = (batchsize*inputsize)\n",
    "        # RNNCell hidden = (batchsize*hiddensize)\n",
    "        hidden = self.rnncell(input, hidden)\n",
    "        return hidden\n",
    "\n",
    "    #初始化零向量作为h0，只有此处用到batch_size\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.batch_size, self.hidden_size)    \n",
    "\n",
    "net = Model(input_size, hidden_size, batch_size)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "    #损失及梯度置0，创建前置条件h0\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    hidden = net.init_hidden()\n",
    "\n",
    "    print(\"Predicted string: \",end=\"\")\n",
    "    #inputs=（seqLen*batchsize*input_size） labels = (seqLen*1)\n",
    "    #input是按序列取的inputs元素（batchsize*inputsize）\n",
    "    #label是按序列去的labels元素（1）\n",
    "    for input, label in zip(inputs, labels):\n",
    "        hidden = net(input, hidden)\n",
    "        #序列的每一项损失都需要累加\n",
    "        loss = + criterion(hidden, label)\n",
    "        #多分类取最大\n",
    "        _, idx = hidden.max(dim=1)\n",
    "        print(idx2char[idx.item()], end='')\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\", Epoch [%d/15] loss = %.4f\" % (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa209b",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bd242a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string:  eelll, Epoch [1/15] loss = 1.484\n",
      "Predicted string:  ellll, Epoch [2/15] loss = 1.318\n",
      "Predicted string:  ellll, Epoch [3/15] loss = 1.194\n",
      "Predicted string:  ellll, Epoch [4/15] loss = 1.102\n",
      "Predicted string:  ollll, Epoch [5/15] loss = 1.029\n",
      "Predicted string:  ollol, Epoch [6/15] loss = 0.965\n",
      "Predicted string:  ohlol, Epoch [7/15] loss = 0.907\n",
      "Predicted string:  ohlol, Epoch [8/15] loss = 0.856\n",
      "Predicted string:  ohlol, Epoch [9/15] loss = 0.810\n",
      "Predicted string:  ohlol, Epoch [10/15] loss = 0.767\n",
      "Predicted string:  ohlol, Epoch [11/15] loss = 0.728\n",
      "Predicted string:  ohlol, Epoch [12/15] loss = 0.694\n",
      "Predicted string:  ohlol, Epoch [13/15] loss = 0.665\n",
      "Predicted string:  ohlol, Epoch [14/15] loss = 0.642\n",
      "Predicted string:  ohlol, Epoch [15/15] loss = 0.624\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 4\n",
    "batch_size = 1\n",
    "num_layers = 1\n",
    "seq_len = 5\n",
    "\n",
    "#构建输入输出字典\n",
    "idx2char = ['e', 'h', 'l', 'o'] # 字典\n",
    "x_data = [1, 0, 2, 2, 3] # hello\n",
    "y_data = [3, 1, 2, 3, 2] # ohlol\n",
    "# y_data = [3, 1, 2, 2, 3]\n",
    "\n",
    "one_hot_lookup = [[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]\n",
    "\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
    "\n",
    "inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size, input_size)\n",
    "#labels（seqLen*batchSize,1）为了之后进行矩阵运算，计算交叉熵\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_size = batch_size #构造H0\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = torch.nn.RNN(input_size = self.input_size,\n",
    "                                hidden_size = self.hidden_size,\n",
    "                                num_layers=num_layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = torch.zeros(self.num_layers,\n",
    "                             self.batch_size,\n",
    "                             self.hidden_size)\n",
    "        out, _ = self.rnn(input, hidden)\n",
    "        #reshape成（SeqLen*batchsize,hiddensize）便于在进行交叉熵计算时可以以矩阵进行。\n",
    "        return out.view(-1, self.hidden_size)\n",
    "\n",
    "net = Model(input_size, hidden_size, batch_size, num_layers)\n",
    "\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.05)\n",
    "\n",
    "#RNN中的输入（SeqLen*batchsize*inputsize）\n",
    "#RNN中的输出（SeqLen*batchsize*hiddensize）\n",
    "#labels维度 hiddensize*1\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, idx = outputs.max(dim=1)\n",
    "    idx = idx.data.numpy()\n",
    "    print('Predicted string: ',''.join([idx2char[x] for x in idx]), end = '')\n",
    "    print(\", Epoch [%d/15] loss = %.3f\" % (epoch+1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e443edd",
   "metadata": {},
   "source": [
    "\n",
    "独热编码在实际问题中容易引起很多问题：\n",
    "\n",
    "    独热编码向量维度过高，每增加一个不同的数据，就要增加一维   \n",
    "    独热编码向量稀疏，每个向量是一个为1其余为0   \n",
    "    独热编码是硬编码，编码情况与数据特征无关   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d050e",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a59933",
   "metadata": {},
   "source": [
    "目的是为了对数据进行降维,增加Embedding层实现降维，增加线性层使之在处理输入输出维度不同的情况下更加稳定。其中的Embedding层的输入必须是LongTensor类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91758f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted string:  eeeee, Epoch [1/15] loss = 1.476\n",
      "Predicted string:  ohlol, Epoch [2/15] loss = 1.121\n",
      "Predicted string:  ohlll, Epoch [3/15] loss = 0.913\n",
      "Predicted string:  ohlol, Epoch [4/15] loss = 0.726\n",
      "Predicted string:  ohlol, Epoch [5/15] loss = 0.554\n",
      "Predicted string:  ohlol, Epoch [6/15] loss = 0.418\n",
      "Predicted string:  ohlol, Epoch [7/15] loss = 0.322\n",
      "Predicted string:  ohlol, Epoch [8/15] loss = 0.255\n",
      "Predicted string:  ohlol, Epoch [9/15] loss = 0.201\n",
      "Predicted string:  ohlol, Epoch [10/15] loss = 0.155\n",
      "Predicted string:  ohlol, Epoch [11/15] loss = 0.116\n",
      "Predicted string:  ohlol, Epoch [12/15] loss = 0.086\n",
      "Predicted string:  ohlol, Epoch [13/15] loss = 0.067\n",
      "Predicted string:  ohlol, Epoch [14/15] loss = 0.054\n",
      "Predicted string:  ohlol, Epoch [15/15] loss = 0.041\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_size = 4\n",
    "num_class = 4\n",
    "hidden_size = 8\n",
    "embedding_size =10     # 数据嵌入层10维空间\n",
    "batch_size = 1\n",
    "num_layers = 2\n",
    "seq_len = 5\n",
    "\n",
    "idx2char_1 = ['e', 'h', 'l', 'o']\n",
    "# idx2char_2 = ['h', 'l', 'o']\n",
    "\n",
    "x_data = [[1, 0, 2, 2, 3]]#(batch,seq_len)\n",
    "y_data = [3, 1, 2, 3, 2] # ohlol#(batch_size*seq_len)\n",
    "\n",
    "#inputs 作为交叉熵中的Inputs，维度为（batchsize，seqLen）\n",
    "inputs = torch.LongTensor(x_data)\n",
    "#labels 作为交叉熵中的Target，维度为（batchsize*seqLen）\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self .emb = torch.nn.Embedding(input_size, embedding_size) # （输入大小*embedding_size）\n",
    "\n",
    "        self.rnn = torch.nn.RNN(input_size = embedding_size,\n",
    "                                hidden_size = hidden_size,\n",
    "                                num_layers=num_layers,\n",
    "                                batch_first = True\n",
    "                               )\n",
    "                                \n",
    "        self.fc = torch.nn.Linear(hidden_size, num_class)\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros(num_layers, x.size(0), hidden_size)\n",
    "        x = self.emb(x)# batch_size,seq_len,embedding_size\n",
    "        x, _ = self.rnn(x, hidden)\n",
    "        x = self.fc(x)\n",
    "        return x.view(-1, num_class)\n",
    "\n",
    "net = Model()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.05)\n",
    "\n",
    "for epoch in range(15):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, idx = outputs.max(dim=1)\n",
    "    idx = idx.data.numpy()\n",
    "    print('Predicted string: ',''.join([idx2char_1[x] for x in idx]), end = '')\n",
    "    print(\", Epoch [%d/15] loss = %.3f\" % (epoch+1, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
